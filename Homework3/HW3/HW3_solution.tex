%%
%% Class homework & solution template for latex
%% Alex Ihler
%%
\documentclass[twoside,11pt]{article}
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{graphicx,color}
\usepackage{verbatim,url}
\usepackage{listings}
\usepackage{upquote}
\usepackage[T1]{fontenc}
%\usepackage{lmodern}
\usepackage[scaled]{beramono}
%\usepackage{textcomp}

% Directories for other source files and images
\newcommand{\bibtexdir}{../bib}
\newcommand{\figdir}{fig}

\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\matlab}{{\sc Matlab}\ }

\setlength{\textheight}{9in} \setlength{\textwidth}{6.5in}
\setlength{\oddsidemargin}{-.25in}  % Centers text.
\setlength{\evensidemargin}{-.25in} %
\setlength{\topmargin}{0in} %
\setlength{\headheight}{0in} %
\setlength{\headsep}{0in} %

\renewcommand{\labelenumi}{(\alph{enumi})}
\renewcommand{\labelenumii}{(\arabic{enumii})}

\theoremstyle{definition}
\newtheorem{MatEx}{M{\scriptsize{ATLAB}} Usage Example}

\definecolor{comments}{rgb}{0,.5,0}
\definecolor{backgnd}{rgb}{.95,.95,.95}
\definecolor{string}{rgb}{.2,.2,.2}
\lstset{language=Matlab}
\lstset{basicstyle=\small\ttfamily,
        mathescape=true,
        emptylines=1, showlines=true,
        backgroundcolor=\color{backgnd},
        commentstyle=\color{comments}\ttfamily, %\rmfamily,
        stringstyle=\color{string}\ttfamily,
        keywordstyle=\ttfamily, %\normalfont,
        showstringspaces=false}
\newcommand{\matp}{\mathbf{\gg}}




\begin{document}

\centerline{\Large Homework 3}
\centerline{Zachary DeStefano, 15247592}
\centerline{CS 273A: Winter 2015}
\centerline{\bf Due: January 27, 2015}

\section*{Problem 1}

\subsection*{Part a}

This is the plot of class 0 versus class 1, which is linearly separable. \\
\begin{figure}[h]
\centering
\includegraphics[width=4 in]{prob1aPlot1.png}
\caption{Class 0 and Class 1 Points}
\end{figure}
\newpage
This is the plot of class 1 versus class 2, which is not linearly separable. \\
\begin{figure}[h]
\centering
\includegraphics[width=4 in]{prob1aPlot2.png}
\caption{Class 1 and Class 2 Points}
\end{figure}
\\
Here is the code to complete part a
\lstinputlisting[firstline=1, lastline=22]{prob1.m}
\newpage
\subsection*{Part b}

Here is the plot of class 0 and class 1 along with the specified decision boundary\\
\\
\begin{figure}[h]
\centering
\includegraphics[width=4 in]{prob1bPlot1.png}
\caption{Class 0 and Class 1 points along with the sample decision boundary}
\end{figure}
\newpage
Here is the plot of class 1 and class 2 along with that decision boundary\\
\\
\begin{figure}[h]
\centering
\includegraphics[width=4 in]{prob1bPlot2.png}
\caption{Class 1 and Class 2 points along with the sample decision boundary}
\end{figure}
\\
Here is the code for plot2DLinear
\lstinputlisting[firstline=1, lastline=21]{@logisticClassify2/plot2DLinear.m}
\newpage
Here is the code to complete part b. It does rely on some of the variables from the part a code.
\lstinputlisting[firstline=24, lastline=46]{prob1.m}

\subsection*{Part c}

I got an error rate of $0.0505$ for data set A\\
 and an error rate of $0.4646$ for data set B.\\
\\
Here is the code for predict.m
\lstinputlisting[firstline=1, lastline=20]{@logisticClassify2/predict.m}
\newpage
As a verification, here are the plots when running plotClassify2D. As can be seen, they are identical to the plots generated by plot2DLinear earlier.
\begin{figure}[h]
\centering
\includegraphics[width=3.5 in]{prob1cPlotA.png}
\caption{Class 0 and 1 points}
\end{figure}
\begin{figure}[h]
\centering
\includegraphics[width=3.5 in]{prob1cPlotB.png}
\caption{Class 1 and 2 points}
\end{figure}

\newpage
Here is the code to compute the error rates and generate the plots for part C. It relies on the code for Part B.
\lstinputlisting[firstline=49, lastline=62]{prob1.m}

\subsection*{Part d}

It can be observed that
\[
\sigma'(z) = \frac{exp(-z)}{(1+exp(-z))^2} = \frac{exp(-z)}{1+exp(-z)} \frac{1}{1+exp(-z)} = (1-\frac{1}{1+exp(-z)})\frac{1}{1+exp(-z)}
\]
We have now derived the following identity
\[
\sigma'(z) = \sigma(z)(1-\sigma(z))
\]
When we first take the derivative, after using the above identity, we end up with
\[
\frac{\partial J_j}{\partial \theta_i} = -y^{(j)} (1-\sigma(z)) z' + (1-y^{(j)}) \sigma(z) z' + 2 \alpha \theta_i
\]
We know that $z' = \frac{\partial z}{\partial \theta_i}= x_i^{(j)}$ thus expanding terms we get
\[
\frac{\partial J_j}{\partial \theta_i} = -y^{(j)}x_i^{(j)} + x_i^{(j)}\sigma(z)y^{(j)} + \sigma(z) x_i^{(j)} -y^{(j)} \sigma(z) x_i^{(j)} + 2 \alpha \theta_i
\]
Cancelling out terms and then putting the $x_i^{(j)}$ factors together, we get
\[
\frac{\partial J_j}{\partial \theta_i} = x_i^{(j)} (\sigma(z) - y^{(j)} ) + 2 \alpha \theta_i
\]
The gradient vector $\nabla J_j(\theta)$ will be as follows
\[
[\frac{\partial J_j}{\partial \theta_1} \frac{\partial J_j}{\partial \theta_2} ... \frac{\partial J_j}{\partial \theta_d}]
\]
\newpage
\subsection*{Part e}

Here is the code for train.m
\lstinputlisting[firstline=1, lastline=117]{@logisticClassify2/train.m}

\newpage

\subsection*{Part f}

For Data Set A, the convergence seemed to work pretty well with the step size already given that starts at 1 and then decreases with each iteration, so I did not touch the step size. I decided to look at number of iterations and stop tolerance. After about 40-50 iterations, it seemed to be converging. When I zoomed in, however, I noticed that the surrogate loss was still decreasing. I decided then to do 100 iterations total and set the stop tolerance really low at 0.0000001 in order to get the surrogate loss as low as possible. In the end, all the points seemed to be classified perfectly which is what we wanted here since the data was linearly separable. \\
\\
Here is the surrogate loss and error rate plot for Data Set A\\
\begin{figure}[h]
\centering
\includegraphics[width=5 in]{prob1fPlotA1.png}
\caption{The surrogate loss and error rate as function of number of iterations}
\end{figure}

\newpage
Here is the classification plot using the final weights computed to attempt to separate Data Set A.\\

\begin{figure}[h]
\centering
\includegraphics[width=5 in]{prob1fPlotA2.png}
\caption{The predicted and actual classification of points. In this case, they were all predicted correctly!}
\end{figure}

\newpage
For Data Set B, the convergence seemed to work well also so I left the step size as it was for data set A. It did take longer for the surrogate loss to converge, so I set the max number of iterations to 500. I decided to set the stop tolerance to the same value at 0.0000001 in order to get the surrogate loss as low as possible. In this case, it did not execute all iterations before the stop tolerance was reached. After around 200 iterations the stop tolerance criteria was met. The error rate however was still somewhat high. When I executed more iterations as a test, the final error rate did not change, so I kept my 200 iteration result. The final error rate being somewhat higher than previously is likely due to the fact that the data is not linearly separable so we reached a point where the error rate was as low as we could possibly make it. \\
\\
Here is the surrogate loss and error rate plot for Data Set B\\
\begin{figure}[h]
\centering
\includegraphics[width=5 in]{prob1fPlotB1.png}
\caption{The surrogate loss and error rate as function of number of iterations}
\end{figure}

\newpage

Here is the classification plot using the final weights computed to attempt to separate Data Set B\\

\begin{figure}[h]
\centering
\includegraphics[width=5 in]{prob1fPlotB2.png}
\caption{The predicted and actual classification of points}
\end{figure}

As far as code is concered, in addition to the train.m function that I listed in part e as well as the code from part a to initialize the variables, I also used the following script to call the train functions and generate the classification plot.
\lstinputlisting[firstline=65, lastline=79]{prob1.m}

\newpage

\section*{Problem 2}

\subsection*{Part a}

This equation describes a vertical line through the points.\\
It will thus easily separate the points in (a) and (b).\\
\\
For the points in (c), if the points (2,2) and (6,4) had one class and the point (4,8) was of a different class, then a vertical line could not separate them.\\
\\
Thus the most points that this one could shatter is 2\\
The VC dimension is thus estimated at 2

\subsection*{Part b}

This equation describes a circle with an arbitrary center and radius. In this case, the inside of the circle must always be assigned negatively and the outside would be assigned positively.\\
\\
The circle will thus easily separate points in (a) and (b)\\
\\
With (c), if all points have the same class then of course this equation will separate them. \\
If one point is negative, then I can draw a small circle around it and the data has been separated. \\
If two points are negative, then we could draw a large circle whose origin is not in the direction of the other point. The two negative points would lie inside the large circle but near the boundary. We have now separated the data.\\
Thus in all cases, we could separate the data in (c)\\
\\
With (d), this equation cannot separate them\\
If the points (2,2) and (8,7) are assigned to be negative and the others positive, then there is no way to draw a circle that would perfectly classify the points. This is because any circle surrounding the negative points would include at least one of the positive points. \\
\\
We have thus estimated that we can only shatter 3 points.\\
The VC dimension is thus estimated at 3.
\newpage
\subsection*{Part c}

This is a line that is centered at the origin and not horizontal since $a \neq 0$. \\
\\
It can easily separate points in (a) and (b)\\
\\
With (c), if the points (6,4) and (4,8) are part of the negative class and the point (2,2) is part of the positive class, then there would be no way to separate them using a line at the origin. In the first quadrant, if a line $L$ passes through the origin, then every point above $L$ makes a slope with the origin that is greater than $L$ and every point below $L$ makes a slope with the origin that is less than $L$. The negative class points will make slopes with the origin of $\frac{2}{3}$ and 2 respectively. The positive class point makes a slope with the origin that is equal to 1. We thus need to find a slope $s$ such that $s \geq 1$ and $s < \frac{2}{3}$ and $s<2$. This is obviously impossible thus we cannot shatter the points in (c).\\
\\
Thus, the most number of points this equation can shatter is 2\\
The VC dimension is thus 2

\end{document}
