%%
%% Class homework & solution template for latex
%% Alex Ihler
%%
\documentclass[twoside,11pt]{article}
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{graphicx,color}
\usepackage{verbatim,url}
\usepackage{listings}
\usepackage{upquote}
\usepackage[T1]{fontenc}
%\usepackage{lmodern}
\usepackage[scaled]{beramono}
%\usepackage{textcomp}

% Directories for other source files and images
\newcommand{\bibtexdir}{../bib}
\newcommand{\figdir}{fig}

\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\matlab}{{\sc Matlab}\ }

\setlength{\textheight}{9in} \setlength{\textwidth}{6.5in}
\setlength{\oddsidemargin}{-.25in}  % Centers text.
\setlength{\evensidemargin}{-.25in} %
\setlength{\topmargin}{0in} %
\setlength{\headheight}{0in} %
\setlength{\headsep}{0in} %

\renewcommand{\labelenumi}{(\alph{enumi})}
\renewcommand{\labelenumii}{(\arabic{enumii})}

\theoremstyle{definition}
\newtheorem{MatEx}{M{\scriptsize{ATLAB}} Usage Example}

\definecolor{comments}{rgb}{0,.5,0}
\definecolor{backgnd}{rgb}{.95,.95,.95}
\definecolor{string}{rgb}{.2,.2,.2}
\lstset{language=Matlab}
\lstset{basicstyle=\small\ttfamily,
        mathescape=true,
        emptylines=1, showlines=true,
        backgroundcolor=\color{backgnd},
        commentstyle=\color{comments}\ttfamily, %\rmfamily,
        stringstyle=\color{string}\ttfamily,
        keywordstyle=\ttfamily, %\normalfont,
        showstringspaces=false}
\newcommand{\matp}{\mathbf{\gg}}




\begin{document}

\centerline{\Large Homework 3}
\centerline{Zachary DeStefano, 15247592}
\centerline{CS 273A: Winter 2015}
\centerline{\bf Due: January 27, 2015}

\section*{Problem 1}

\subsection*{Part a}

This is the plot of class 0 versus class 1, which is linearly separable. \\
\begin{figure}[h]
\centering
\includegraphics[width=4 in]{prob1aPlot1.png}
\caption{Class 0 and Class 1 Points}
\end{figure}
\newpage
This is the plot of class 1 versus class 2, which is not linearly separable. \\
\begin{figure}[h]
\centering
\includegraphics[width=4 in]{prob1aPlot2.png}
\caption{Class 1 and Class 2 Points}
\end{figure}
\\
Here is the code to complete part a
\lstinputlisting[firstline=1, lastline=22]{prob1.m}
\newpage
\subsection*{Part b}

Here is the plot of class 0 and class 1 along with the specified decision boundary\\
\\
\begin{figure}[h]
\centering
\includegraphics[width=4 in]{prob1bPlot1.png}
\caption{Class 0 and Class 1 points along with the sample decision boundary}
\end{figure}
\newpage
Here is the plot of class 1 and class 2 along with that decision boundary\\
\\
\begin{figure}[h]
\centering
\includegraphics[width=4 in]{prob1bPlot2.png}
\caption{Class 1 and Class 2 points along with the sample decision boundary}
\end{figure}
\\
Here is the code for plot2DLinear
\lstinputlisting[firstline=1, lastline=21]{@logisticClassify2/plot2DLinear.m}
\newpage
Here is the code to complete part b. It does rely on some of the variables from the part a code.
\lstinputlisting[firstline=24, lastline=46]{prob1.m}

\subsection*{Part c}

I got an error rate of $0.0505$ for data set A\\
 and an error rate of $0.4646$ for data set B.\\
\\
Here is the code for predict.m
\lstinputlisting[firstline=1, lastline=20]{@logisticClassify2/predict.m}
\newpage
Here is the code to compute the error rates for part C. It relies on the code for Part B.
\lstinputlisting[firstline=46, lastline=57]{prob1.m}

\subsection*{Part d}

We will use the following identity:
\[
\sigma'(z) = \sigma(z)(1-\sigma(z))
\]
When we first take the derivative, we end up with
\[
\frac{\partial J_j}{\partial \theta_i} = -y^{(j)} (1-\sigma(z)) z' + (1-y^{(j)}) \sigma(z) z' + 2 \alpha \theta_i
\]
We know that $z' = \frac{\partial z}{\partial \theta_i}= x_i^{(j)}$ thus expanding terms we get
\[
\frac{\partial J_j}{\partial \theta_i} = -y^{(j)}x_i^{(j)} + x_i^{(j)}\sigma(z)y^{(j)} + \sigma(z) x_i^{(j)} -y^{(j)} \sigma(z) x_i^{(j)} + 2 \alpha \theta_i
\]
Cancelling out terms and then putting the $x_i^{(j)}$ factors together, we get
\[
\frac{\partial J_j}{\partial \theta_i} = x_i^{(j)} (\sigma(z) - y^{(j)} ) + 2 \alpha \theta_i
\]
The gradient vector $\nabla J_j(\theta)$ will be as follows
\[
[\frac{\partial J_j}{\partial \theta_1} \frac{\partial J_j}{\partial \theta_2} ... \frac{\partial J_j}{\partial \theta_d}]
\]
\newpage
\subsection*{Part e}

Here is the code for train.m
\lstinputlisting[firstline=1, lastline=117]{@logisticClassify2/train.m}

\subsection*{Part f}

For Data Set A, the convergence seemed to work pretty well with the step size already given that starts at 1 and then decreases with each iteration, so I did not touch the step size. I decided to look at number of iterations and stop tolerance. After about 30-40 iterations, it seemed to be converging. When I zoomed in, however, I noticed that the error rate was still decreasing. I decided then to do 100 iterations total and set the stop tolerance really low at 0.0000001 in order to get the error rate as low as possible. In the end, all the points seemed to be classified perfectly which is what we wanted here since the data was linearly separable. \\
\\
Here are the plots for Data Set A
\begin{figure}[h]
\centering
\includegraphics[width=6 in]{prob1fPlotA1.png}
\caption{The surrogate loss and error rate as function of number of iterations}
\end{figure}
\begin{figure}[h]
\centering
\includegraphics[width=6 in]{prob1fPlotA2.png}
\caption{The predicted and actual classification of points. In this case, they were all predicted correctly!}
\end{figure}

\section*{Problem 2}

\subsection*{Part a}

It can separate the points in a,b, and c, but you can arrange the points in d such that this won't separate them. The VC dimension is thus 3.

\subsection*{Part b}

It can separate the points in a,b,c, and d, so the VC dimension is 4.

\end{document}
